"""
Este script realiza a importaÃ§Ã£o de dados de arquivos Excel para um banco de dados PostgreSQL. 
Ele foi projetado para processar arquivos especÃ­ficos, mapear tabelas no banco de dados e 
realizar a inserÃ§Ã£o de dados de forma eficiente. Abaixo estÃ¡ uma descriÃ§Ã£o detalhada das 
principais funÃ§Ãµes e do fluxo de execuÃ§Ã£o:

Considerando o processo de extraÃ§Ã£o desenvolvido para mais Cias, este mÃ³dulo deverÃ¡ atender univeralmente a call do extrator, seja qual for a CIA;

FunÃ§Ãµes principais:
-------------------
1. create_database_connection():
    - Estabelece uma conexÃ£o com o banco de dados PostgreSQL utilizando variÃ¡veis de ambiente 
      para configurar os parÃ¢metros de conexÃ£o.
2. sanitize_table_name(table_name):
    - Normaliza o nome da tabela removendo caracteres invÃ¡lidos, garantindo que o nome seja 
      compatÃ­vel com o banco de dados.
3. get_table_mapping():
    - ObtÃ©m a lista de tabelas mapeadas a partir de uma variÃ¡vel de ambiente e retorna uma 
      lista de nomes de tabelas sanitizados.
4. get_column_types(cursor, table_name):
    - Recupera os tipos de dados das colunas de uma tabela no banco de dados, utilizando 
      a tabela de metadados `information_schema.columns`.
5. convert_df_to_db_schema(df, column_types):
    - Converte os tipos de dados de um DataFrame do pandas para corresponder aos tipos de 
      dados esperados no banco de dados.
6. import_data_to_db(file_dfs):
    - Realiza a importaÃ§Ã£o dos dados de mÃºltiplos DataFrames para as tabelas correspondentes 
      no banco de dados. Inclui validaÃ§Ã£o de colunas, filtragem e inserÃ§Ã£o em lote para 
      melhorar a performance.
7. process_files(root_folder_path):
    - Processa os arquivos Excel em um diretÃ³rio especÃ­fico, sanitiza os nomes das colunas 
      e prepara os dados para importaÃ§Ã£o.
Fluxo de execuÃ§Ã£o:
------------------
1. O script comeÃ§a carregando as variÃ¡veis de ambiente e definindo o caminho da pasta raiz 
    onde os arquivos Excel estÃ£o localizados.
2. A funÃ§Ã£o `process_files()` Ã© chamada para processar os arquivos Excel no diretÃ³rio 
    especificado. Apenas arquivos que seguem o padrÃ£o "CONSOLIDADO-" sÃ£o considerados.
3. Os dados processados sÃ£o armazenados em um dicionÃ¡rio, onde as chaves sÃ£o os nomes das 
    tabelas e os valores sÃ£o os DataFrames correspondentes.
4. A funÃ§Ã£o `import_data_to_db()` Ã© chamada para importar os dados processados para o banco 
    de dados. Durante a importaÃ§Ã£o, sÃ£o realizadas validaÃ§Ãµes de colunas e conversÃµes de tipos.
5. O script exibe mensagens de progresso e erros durante o processamento e a importaÃ§Ã£o, 
    garantindo que o usuÃ¡rio seja informado sobre o status da execuÃ§Ã£o.
6. Ao final, o script informa se a importaÃ§Ã£o foi concluÃ­da com sucesso ou se ocorreram erros.
Requisitos:
-----------
- Python 3.6 ou superior
- Bibliotecas: psycopg2, pandas, dotenv, tqdm, openpyxl
- Banco de dados PostgreSQL configurado e acessÃ­vel
- VariÃ¡veis de ambiente configuradas para conexÃ£o com o banco de dados e mapeamento de tabelas
Como executar:
--------------
1. Configure as variÃ¡veis de ambiente necessÃ¡rias no arquivo `.env`.
2. Certifique-se de que os arquivos Excel estejam no diretÃ³rio especificado.
3. Execute o script diretamente para iniciar o processamento e a importaÃ§Ã£o.
##-- Validar se a independÃªncia entre input x extraÃ§Ã£o deve ser preservada independente de manutenÃ§Ã£o; 

========================================================================================================================================
*******  O processo abaixo esta desenhado com base em unificaÃ§Ã£o dos arquivos, prÃ³ximo passo para este mÃ³dulo Ã© preparar o ambiente para apenas corresponder
Ã  importaÃ§Ã£o dos arquivos extraÃ­dos, independente de unificaÃ§Ã£o ou nÃ£o e independente do start ser via call do extrator ou inicio programado ******

Herdar do extrator a construÃ§ao de cia e tabela no .env, para que o processo de importaÃ§Ã£o seja feito sequencialmente ao extrator, tanto faseado quanto independentemente.
========================================================================================================================================

"""
from dotenv import load_dotenv
import os
from tqdm import tqdm
from typing import Dict, Any, Tuple, List, Optional
from extrato_app.CoreData.grande_conn import DatabaseManager
from extrato_app.CoreData.dba import DBA
from extrato_app.CoreData.data_handler import DataHandler
from extrato_app.CoreData.consolidador import Consolidador
import json
from extrato_app.CoreData.ds4 import parse_meses_opt, escolher_cia_e_atualizar_config, obter_mes_ano
from pathlib import Path
import pandas as pd
import time
import msoffcrypto
from io import BytesIO


load_dotenv(dotenv_path=os.path.join(os.getcwd(), '.env'))

ROOT_NUMS = os.getenv("ROOT_NUMS", "")
MESES_PT = parse_meses_opt(os.getenv("MESES_OPT", ""))

def obter_mes_ano_from_config() -> Tuple[int, int]:
    config_path = os.path.join(os.getcwd(), "config.json")
    if os.path.exists(config_path):
        with open(config_path, "r", encoding="utf-8") as f:
            try:
                config = json.load(f)
                competencia = config.get("competencia", "")
                if competencia:
                    return obter_mes_ano(competencia)
            except Exception:
                pass
    raise ValueError("CompetÃªncia nÃ£o definida nem no parÃ¢metro nem no config.json")

class DataImporter:
    def __init__(self, cia_manual: Optional[str] = None, competencia_manual: Optional[str] = None):
        if not ROOT_NUMS:
            print("ğŸš¨ ROOT_NUMS nÃ£o encontrado no .env")
            return
        if not MESES_PT:
            print("ğŸš¨ MESES_OPT nÃ£o encontrado no .env")
            return

        if cia_manual:
            self.cia_escolhida = cia_manual
        else:
            self.cia_escolhida = escolher_cia_e_atualizar_config()
        
        if not self.cia_escolhida:
            return
        
        mes, ano = obter_mes_ano(competencia_manual) if competencia_manual else obter_mes_ano_from_config()
        
        self.root_path = os.path.join(
            ROOT_NUMS,
            str(ano),
            "Controle de produÃ§Ã£o",
            f"{mes} - {MESES_PT[mes]}",
            self.cia_escolhida
        )
        
        print(f"\nğŸ“‚ Caminho selecionado: {self.root_path}")
        
        self.dba = DBA()
        self.data_handler = DataHandler()
        self.consolidador = Consolidador()

    def import_data_to_db(self, processed_data: Dict[str, Any], id_cia: str) -> bool:

        print('Iniciando importaÃ§Ã£o principal...')
        overall_success = True
        
        for table_name, data in processed_data.items():
            print(f"ğŸ“Š Importando {table_name}...")
            conn = DatabaseManager.get_connection()
            try:
                success = self.dba.import_main(
                    conn=conn,
                    df_filtered=data['df'],
                    table_name=table_name,
                    ordered_cols=data['ordered_cols'],
                    ordered_cols_escaped=data['ordered_cols_escaped']
                )
                if success is not None:
                    overall_success &= success
                else:
                    print(f"âš ï¸ A importaÃ§Ã£o de {table_name} retornou None, considerado como falha")
                    overall_success = False
            finally:
                DatabaseManager.return_connection(conn)
        
        return overall_success

    def execute_pipeline(self) -> Tuple[bool, Optional[Dict[str, Any]]]:
        start = time.perf_counter()
        print(f"ğŸ“‚ Acessando pasta: {self.root_path}")
        file_dfs = self.data_handler.process_files(self.root_path)
        if not file_dfs:
            print("ğŸš¨ Nenhum dado vÃ¡lido encontrado")
            return False, None

        print("\nğŸ” Validando CIAs...")
        existing_cias, non_existing_cias, cias_list, id_cia = self.dba.get_and_compare_cias()
        if not id_cia:
            print("ğŸš¨ Nenhum ID de CIA vÃ¡lido encontrado")
            return False, None

        print("\nâš™ï¸ Processando dados...")
        processed_data = self.data_handler.treat_zero(file_dfs, id_cia)

        print("\nğŸš€ Iniciando importaÃ§Ã£o...")
        import_success = self.import_data_to_db(processed_data, id_cia)

        print("\nğŸ’¾ Exportando dados para Excel...")
        export_success = self.data_handler.export_to_excel(processed_data)
        
        overall_success = import_success and export_success

        if overall_success:
            print("\nğŸ‰ Pipeline concluÃ­do com sucesso!")
        else:
            if not import_success:
                print("\nâš ï¸ ImportaÃ§Ã£o concluÃ­da com erros")
            if not export_success:
                print("\nâš ï¸ ExportaÃ§Ã£o para Excel concluÃ­da com erros")


        end = time.perf_counter()
        print(f"â±ï¸ Tempo de leitura otimizada: {end - start:.2f}s")


        return overall_success, processed_data



def main(cia_manual: Optional[str] = None, competencia_manual: Optional[str] = None):
    importer = DataImporter(cia_manual, competencia_manual)
    return importer.execute_pipeline()

if __name__ == "__main__":
    main()
